{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook uses the VGG 16 pretrained weights and predict an image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Various classes of the Imagenet are mentioned here\n",
    "https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define the VggNet architecture and return the model\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Flatten, Dense, Dropout\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.optimizers import SGD\n",
    "import numpy as np\n",
    "\n",
    "# define a VGG16 network\n",
    "def VGG_16(weights_path=None):\n",
    "    model = Sequential()\n",
    "    model.add(ZeroPadding2D((1,1),input_shape=(3,224,224)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "    model.add(Flatten())\n",
    "    #top layer of the VGG net\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1000, activation='softmax'))\n",
    "    if weights_path:\n",
    "        model.load_weights(weights_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a test image and pass it on to the VGG16 model to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 224, 3)\n",
      "(3, 224, 224)\n",
      "(1, 3, 224, 224)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The shape of the input to \"Flatten\" is not fully defined (got (0, 7, 512). Make sure to pass a complete \"input_shape\" or \"batch_input_shape\" argument to the first layer in your model.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f71f26ecbc46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mmodel1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVGG_16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/tkmacl9/Desktop/FastAIDLCourse/nbs/KerasDeepLearning/vgg16_weights.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sparse_categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-68224a97039b>\u001b[0m in \u001b[0;36mVGG_16\u001b[0;34m(weights_path)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;31m#top layer of the VGG net\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4096\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/tkmacl9/anaconda/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    464\u001b[0m                           output_shapes=[self.outputs[0]._keras_shape])\n\u001b[1;32m    465\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m                 raise TypeError('All layers in a Sequential model '\n",
      "\u001b[0;32m/Users/tkmacl9/anaconda/lib/python3.5/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;31m# Infering the output shape is only relevant for Theano.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_to_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m                 \u001b[0moutput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    605\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/tkmacl9/anaconda/lib/python3.5/site-packages/keras/layers/core.py\u001b[0m in \u001b[0;36mcompute_output_shape\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    486\u001b[0m             raise ValueError('The shape of the input to \"Flatten\" '\n\u001b[1;32m    487\u001b[0m                              \u001b[0;34m'is not fully defined '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m                              \u001b[0;34m'(got '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m                              \u001b[0;34m'Make sure to pass a complete \"input_shape\" '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m                              \u001b[0;34m'or \"batch_input_shape\" argument to the first '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The shape of the input to \"Flatten\" is not fully defined (got (0, 7, 512). Make sure to pass a complete \"input_shape\" or \"batch_input_shape\" argument to the first layer in your model."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.misc\n",
    "from keras.models import model_from_json\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "base_location = '/Users/tkmacl9/Desktop/FastAIDLCourse/nbs/KerasDeepLearning/'\n",
    "img_name = base_location+'cat_standing.jpg'\n",
    "#img_names = [base_location+'cat_standing.jpg', base_location+'dog.jpg', base_location+'horse_1.jpg']\n",
    "#imgs = [scipy.misc.imresize(scipy.misc.imread(img_name), (224,224)).astype('float32') for img_name in img_names]\n",
    "#imgs = [np.transpose(scipy.misc.imresize(scipy.misc.imread(img_name), (32,32)), (1,0,2)).astype('float32') for img_name in img_names]\n",
    "#imgs = np.array(imgs) / 255\n",
    "img = scipy.misc.imresize(scipy.misc.imread(img_name), (224,224)).astype('float32')\n",
    "print(img.shape)\n",
    "img = img.transpose((2,0,1))\n",
    "print(img.shape)\n",
    "img = np.expand_dims(img, axis=0)\n",
    "print(img.shape)\n",
    "\n",
    "#train\n",
    "optim=SGD()\n",
    "model1 = VGG_16('/Users/tkmacl9/Desktop/FastAIDLCourse/nbs/KerasDeepLearning/vgg16_weights.h5')\n",
    "model1.compile(optimizer=optim, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#predict\n",
    "predictions = model1.predict(img)\n",
    "print(np.argmax(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Pretrained models which are available in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "from keras.optimizers import SGD\n",
    "from keras.applications.vgg16 import VGG16\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "%matplotlib inline\n",
    "\n",
    "# Use the pretrained Vgg16 model\n",
    "model = VGG16(weights='imagenet', include_top=True)\n",
    "model.summary()\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 224, 3)\n",
      "(1, 224, 224, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11148bac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "866\n"
     ]
    }
   ],
   "source": [
    "#resize into Vgg16 trained image format\n",
    "\n",
    "base_location = '/Users/tkmacl9/Desktop/FastAIDLCourse/nbs/KerasDeepLearning/'\n",
    "img_name = base_location+'steam_engine.jpg'\n",
    "img = scipy.misc.imresize(scipy.misc.imread(img_name), (224,224)).astype('float32')\n",
    "print(img.shape)\n",
    "img = np.expand_dims(img, axis=0)\n",
    "print(img.shape)\n",
    "\n",
    "# predict\n",
    "out = model.predict(img)\n",
    "plt.plot(out.ravel())\n",
    "plt.show()\n",
    "print(np.argmax(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the weights of a particular layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_6 (None, 224, 224, 3)\n",
      "1 block1_conv1 (None, 224, 224, 64)\n",
      "2 block1_conv2 (None, 224, 224, 64)\n",
      "3 block1_pool (None, 112, 112, 64)\n",
      "4 block2_conv1 (None, 112, 112, 128)\n",
      "5 block2_conv2 (None, 112, 112, 128)\n",
      "6 block2_pool (None, 56, 56, 128)\n",
      "7 block3_conv1 (None, 56, 56, 256)\n",
      "8 block3_conv2 (None, 56, 56, 256)\n",
      "9 block3_conv3 (None, 56, 56, 256)\n",
      "10 block3_pool (None, 28, 28, 256)\n",
      "11 block4_conv1 (None, 28, 28, 512)\n",
      "12 block4_conv2 (None, 28, 28, 512)\n",
      "13 block4_conv3 (None, 28, 28, 512)\n",
      "14 block4_pool (None, 14, 14, 512)\n",
      "15 block5_conv1 (None, 14, 14, 512)\n",
      "16 block5_conv2 (None, 14, 14, 512)\n",
      "17 block5_conv3 (None, 14, 14, 512)\n",
      "18 block5_pool (None, 7, 7, 512)\n",
      "19 flatten (None, 25088)\n",
      "20 fc1 (None, 4096)\n",
      "21 fc2 (None, 4096)\n",
      "22 predictions (None, 1000)\n",
      "*********************************\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tkmacl9/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:13: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=flatten_1/..., outputs=InplaceDim...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "=================================================================\n",
      "Total params: 7,635,264\n",
      "Trainable params: 7,635,264\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[[[[   0.            0.            0.         ...,    0.            0.\n",
      "       0.        ]\n",
      "   [   0.            0.            0.         ...,    0.            0.\n",
      "       0.        ]\n",
      "   [   0.            0.            0.         ...,    0.           10.13583851\n",
      "       0.        ]\n",
      "   ..., \n",
      "   [   0.            0.            0.         ...,    0.            0.\n",
      "       0.        ]\n",
      "   [   0.            0.            0.         ...,    8.46161747    0.\n",
      "       0.        ]\n",
      "   [   0.            0.           11.76675415 ...,   17.23648643    0.\n",
      "       0.        ]]\n",
      "\n",
      "  [[   0.            0.            6.26944399 ...,   39.08948898\n",
      "      13.22397137    0.        ]\n",
      "   [   0.            0.            0.         ...,   45.62240219\n",
      "     197.30795288    0.        ]\n",
      "   [   0.            0.            0.         ...,   28.29047966\n",
      "     737.93914795    0.        ]\n",
      "   ..., \n",
      "   [   0.            0.            0.         ...,    0.            0.\n",
      "       0.        ]\n",
      "   [   0.            0.            0.         ...,   53.18551636    0.\n",
      "       0.        ]\n",
      "   [   0.            0.            9.60310745 ...,   45.72634506\n",
      "     342.67681885    0.        ]]\n",
      "\n",
      "  [[   0.            0.           74.27809906 ...,    9.72535133    0.\n",
      "       0.        ]\n",
      "   [   0.            0.           57.10578537 ...,   87.51924133    0.\n",
      "       0.        ]\n",
      "   [   0.            0.            0.         ...,    0.          740.53851318\n",
      "     127.65803528]\n",
      "   ..., \n",
      "   [   0.            0.            0.         ...,    0.            0.\n",
      "       0.        ]\n",
      "   [   0.            0.            0.         ...,    0.            0.\n",
      "       0.        ]\n",
      "   [   0.            0.            0.         ...,    0.          306.69595337\n",
      "       0.        ]]\n",
      "\n",
      "  ..., \n",
      "  [[   0.            0.            0.         ...,    0.           43.55426788\n",
      "       0.        ]\n",
      "   [   0.            0.            0.         ...,    0.            0.\n",
      "       0.        ]\n",
      "   [   0.            0.            0.         ...,    0.            0.\n",
      "       0.        ]\n",
      "   ..., \n",
      "   [   0.            0.            0.         ...,    0.          647.89599609\n",
      "       0.        ]\n",
      "   [   0.            0.            0.         ...,    0.          354.52807617\n",
      "       0.        ]\n",
      "   [   0.            0.            0.         ...,    0.            0.\n",
      "       0.        ]]\n",
      "\n",
      "  [[   0.            0.            0.         ...,    0.           51.07883072\n",
      "       0.        ]\n",
      "   [   0.            0.            0.         ...,   18.04098701    0.\n",
      "       0.        ]\n",
      "   [   0.            0.            0.         ...,    0.            0.\n",
      "       0.        ]\n",
      "   ..., \n",
      "   [   0.            0.            0.         ...,    0.          396.82662964\n",
      "       0.        ]\n",
      "   [   0.            0.            0.         ...,    0.          108.52274323\n",
      "       0.        ]\n",
      "   [   0.            0.            0.         ...,    0.            0.\n",
      "       0.        ]]\n",
      "\n",
      "  [[   0.            0.            7.4079237  ...,   65.90002441\n",
      "      46.50641632    0.        ]\n",
      "   [   0.            0.            0.         ...,   51.99277878\n",
      "      27.33309937    0.        ]\n",
      "   [   0.            0.           24.65453529 ...,   70.27101135    0.\n",
      "       0.        ]\n",
      "   ..., \n",
      "   [   0.            0.            3.71661711 ...,   15.69657135\n",
      "      36.84810638    0.        ]\n",
      "   [   0.            0.            0.         ...,    8.09237671    0.\n",
      "       0.        ]\n",
      "   [   0.            0.            0.         ...,    1.31770456    0.\n",
      "       0.        ]]]]\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import numpy as np\n",
    "\n",
    "# pre-built and pre-trained deep learning VGG16 model\n",
    "base_model = VGG16(weights='imagenet', include_top=True)\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "     print (i, layer.name, layer.output_shape)\n",
    "\n",
    "# extract features from block4_pool block\n",
    "model = Model(input=base_model.input, output=base_model.get_layer('block4_pool').output)\n",
    "print('*********************************')\n",
    "model.summary()\n",
    "\n",
    "#resizing image\n",
    "img_path = base_location+'cat_standing.jpg'\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "# get the features from this block\n",
    "features = model.predict(x)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
