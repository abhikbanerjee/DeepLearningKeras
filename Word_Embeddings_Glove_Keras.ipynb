{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook works on Word Embeddings using Keras (covers Glove models and pre-trained embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "from keras.layers import Merge\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.core import Dense,Reshape, Lambda\n",
    "from keras.models import Sequential\n",
    "import keras.backend as K\n",
    "\n",
    "vocab_size = 5000\n",
    "embedding_size = 300\n",
    "window_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Context model and the Word model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "merge_6 (Merge)              (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 3,000,002\n",
      "Trainable params: 3,000,002\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tkmacl9/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:13: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/Users/tkmacl9/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:14: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, kernel_initializer=\"glorot_uniform\", activation=\"sigmoid\")`\n"
     ]
    }
   ],
   "source": [
    "# word model definition\n",
    "word_model = Sequential()\n",
    "word_model.add(Embedding(vocab_size, embedding_size, embeddings_initializer='glorot_uniform', input_length=1))\n",
    "word_model.add(Reshape((embedding_size,)))\n",
    "\n",
    "# Context model definition\n",
    "context_model = Sequential()\n",
    "context_model.add(Embedding(vocab_size, embedding_size, embeddings_initializer='glorot_uniform', input_length=1))\n",
    "context_model.add(Reshape((embedding_size,)))\n",
    "\n",
    "# add dot product and then a dense layer with sigmoid activation\n",
    "model = Sequential()\n",
    "model.add(Merge([word_model, context_model], mode=\"dot\"))\n",
    "model.add(Dense(1, init=\"glorot_uniform\", activation=\"sigmoid\"))\n",
    "model.compile(loss=\"mean_squared_error\",optimizer=\"adam\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This section reads the embedded layer weights which we are interested in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 300)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_layer = model.layers[0]\n",
    "word_model = merge_layer.layers[0]\n",
    "word_embed_layer = word_model.layers[0]\n",
    "weights = word_embed_layer.get_weights()[0]\n",
    "weights.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This section handles and preprocesses the text to create the skip-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import *\n",
    "from keras.preprocessing.sequence import skipgrams\n",
    "\n",
    "text = \"I love green eggs and ham\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ham': 1, 'eggs': 2, 'and': 3, 'i': 4, 'green': 5, 'love': 6}\n",
      "6\n",
      "{1: 'ham', 2: 'eggs', 3: 'and', 4: 'i', 5: 'green', 6: 'love'}\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# create dictionary from word_to_id and id_to_word\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "word2id = tokenizer.word_index\n",
    "id2word = {v:k for k,v in word2id.items()}\n",
    "print(word2id)\n",
    "print(len(word2id))\n",
    "print(id2word)\n",
    "print(len(id2word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the skipgrams for the given text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 6, 5, 2, 3, 1]\n",
      "length of pairs -  56\n",
      "Length of labels -  56\n",
      "\n",
      "Text is -  I love green eggs and ham\n",
      "\n",
      "love 6  :  ham 1  ->  1\n",
      "and 3  :  eggs 2  ->  0\n",
      "eggs 2  :  green 5  ->  1\n",
      "eggs 2  :  green 5  ->  0\n",
      "eggs 2  :  and 3  ->  0\n",
      "love 6  :  and 3  ->  1\n",
      "love 6  :  green 5  ->  0\n",
      "i 4  :  eggs 2  ->  1\n",
      "and 3  :  ham 1  ->  1\n",
      "love 6  :  i 4  ->  1\n",
      "eggs 2  :  ham 1  ->  0\n",
      "love 6  :  green 5  ->  0\n",
      "i 4  :  eggs 2  ->  0\n",
      "green 5  :  and 3  ->  0\n",
      "eggs 2  :  i 4  ->  1\n",
      "green 5  :  green 5  ->  0\n",
      "ham 1  :  eggs 2  ->  0\n",
      "ham 1  :  ham 1  ->  0\n",
      "ham 1  :  green 5  ->  1\n",
      "love 6  :  and 3  ->  0\n"
     ]
    }
   ],
   "source": [
    "# splits the sentence into list of words and gets the ids for those words from the dict\n",
    "wids = [word2id[w] for w in text_to_word_sequence(text)]\n",
    "print(wids)\n",
    "\n",
    "# create the skipgrams for the texts - the default window_size is 4 for the skipgrams\n",
    "pairs, labels = skipgrams(wids, len(word2id))\n",
    "# pairs, labels = skipgrams(wids, len(word2id), window_size=4)\n",
    "print(\"length of pairs - \", len(pairs))\n",
    "# print(pairs[1])\n",
    "print(\"Length of labels - \", len(labels))\n",
    "print()\n",
    "\n",
    "#print 10 such examples\n",
    "print(\"Text is - \",text)\n",
    "print()\n",
    "for i in range(20):\n",
    "    print(id2word[pairs[i][0]], pairs[i][0],\" : \",\n",
    "         id2word[pairs[i][1]], pairs[i][1],\n",
    "         \" -> \", labels[i] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the CBOW for the given text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_18 (Embedding)     (None, 2, 300)            1500000   \n",
      "_________________________________________________________________\n",
      "lambda_4 (Lambda)            (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 5000)              1505000   \n",
      "=================================================================\n",
      "Total params: 3,005,000\n",
      "Trainable params: 3,005,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_size, embeddings_initializer='glorot_uniform', \n",
    "                   input_length=window_size*2))\n",
    "model.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embedding_size, )))\n",
    "model.add(Dense(vocab_size, kernel_initializer='glorot_uniform', activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights of the embedded layer from the CBOW model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 300)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = model.layers[0].get_weights()[0]\n",
    "weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Third Party implementations of Word2Vec - Use Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import keyedvectors\n",
    "import os\n",
    "import logging\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create sentences of maxlength from the text8 corpus\n",
    "\n",
    "class text2sentences(object):\n",
    "    def __init__(self, fname, maxlength):\n",
    "        self.fname = fname\n",
    "        self.maxlength = maxlength\n",
    "    def __iter__(self):\n",
    "        with open(os.path.join(DATA_DIR, \"text8\"), \"r\") as ftext:\n",
    "            text = ftext.read().split(\" \")\n",
    "            words = []\n",
    "            for word in text:\n",
    "                if len(words) >= self.maxlength:\n",
    "                    yield words\n",
    "                    words = []\n",
    "                words.append(word)\n",
    "            yield words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating sentences using the class defined above of maxlength of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#setting the logging level\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "DATA_DIR = \"/Users/tkmacl9/Desktop/FastAIDLCourse/text8_word2vec/\"\n",
    "sentences = text2sentences(os.path.join(DATA_DIR, \"text8\"), 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model and normalize the model params and save them and load them back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec(sentences, size=300, min_count=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#normalize the model parameters and save the model\n",
    "model.init_sims(replace=True)\n",
    "model.save(\"/Users/tkmacl9/Desktop/FastAIDLCourse/text8_word2vec/word2vec_gensim.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load the saved model\n",
    "model = word2vec.Word2Vec.load(\"/Users/tkmacl9/Desktop/FastAIDLCourse/text8_word2vec/word2vec_gensim.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the loaded model to find similarity score between word pairs, find most similar words, or the embedded vectors of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -1.02323689e-01  -4.41231430e-02   3.95736434e-02  -3.74355279e-02\n",
      "  -7.39789829e-02   2.88131237e-02  -3.40031683e-02   7.72187039e-02\n",
      "  -5.08850664e-02   1.06910309e-02   5.06884567e-02   5.08938171e-02\n",
      "  -1.23202115e-01  -4.53161970e-02  -3.49204540e-02  -1.31303193e-02\n",
      "  -1.43687138e-02   7.93975964e-02   6.11007363e-02   2.59787682e-02\n",
      "  -3.98292840e-02  -3.16743110e-03  -2.58806758e-02  -1.08963745e-02\n",
      "  -1.30261825e-02  -4.39308584e-03  -2.38791425e-02   1.70451522e-01\n",
      "  -8.04974809e-02   6.84786439e-02   7.21806735e-02  -4.48552333e-02\n",
      "   6.35374635e-02  -2.24758815e-02  -6.36098534e-02   6.69187028e-03\n",
      "  -4.98502031e-02  -2.35009082e-02   6.53387234e-02   1.03983983e-01\n",
      "  -4.37662974e-02  -3.59887369e-02  -3.01812459e-02  -7.45489746e-02\n",
      "   3.52005512e-02   8.18186328e-02   7.92362075e-03   1.09489933e-01\n",
      "   4.34352420e-02   5.51684387e-03   8.38563871e-03  -1.33026019e-02\n",
      "  -2.11742762e-02   7.18761533e-02   6.18180856e-02   1.10621611e-03\n",
      "  -7.57084042e-02   1.82901308e-01  -6.06088266e-02   3.81483212e-02\n",
      "  -3.74340899e-02   3.18007693e-02  -6.44317567e-02   1.57347191e-02\n",
      "   3.30715105e-02  -2.11812016e-02  -1.76742114e-02   2.28279661e-02\n",
      "  -1.75720990e-01  -7.20019788e-02  -2.53028627e-02   9.52363387e-02\n",
      "  -7.32888877e-02  -1.31786363e-02   1.04684681e-01   7.63750300e-02\n",
      "  -7.84425884e-02  -1.81127395e-02  -5.78220338e-02  -2.32827123e-02\n",
      "   3.98166776e-02  -3.40152942e-02   4.63855779e-03   2.78051440e-02\n",
      "   7.35282451e-02   2.76517253e-02  -4.17274311e-02   1.47013618e-02\n",
      "  -1.69264171e-02   5.22593111e-02   3.26177627e-02  -4.56192624e-03\n",
      "  -1.14200786e-01  -6.15125559e-02   3.03277746e-02  -4.16748598e-02\n",
      "  -1.08551815e-01   7.04812333e-02   1.38158217e-01  -5.57270041e-03\n",
      "  -4.18259464e-02   7.70049822e-03   8.81085470e-02  -3.43696289e-02\n",
      "  -1.31762652e-02   6.21089246e-03   9.05101479e-04   6.80263489e-02\n",
      "  -6.47568032e-02   5.41782416e-02  -3.10815908e-02   1.62716452e-02\n",
      "   4.34203930e-02  -1.24495558e-01   3.29673737e-02  -7.41678253e-02\n",
      "   3.89091596e-02   4.91028018e-02  -1.38490885e-01   2.08993144e-02\n",
      "   4.75117043e-02   5.31796440e-02   9.04360637e-02   1.17336316e-02\n",
      "   6.76228032e-02   3.22307162e-02  -7.24438112e-03   1.10259674e-01\n",
      "  -2.80760638e-02  -3.43275964e-02   5.54892793e-03  -9.06195194e-02\n",
      "   7.86012411e-02  -8.55570734e-02   6.23092726e-02  -7.79110985e-03\n",
      "  -4.59705889e-02   7.52039924e-02  -4.81599150e-03   4.83137257e-02\n",
      "  -1.07018743e-02  -2.75608059e-02   5.43061504e-03  -1.31425150e-02\n",
      "  -7.51694385e-03  -4.08741310e-02   7.21377581e-02   6.46860944e-03\n",
      "   1.15856335e-01  -7.95382038e-02   5.14474511e-03   1.66361220e-02\n",
      "  -2.62401439e-02   4.65605184e-02  -6.10383460e-03   7.00520119e-03\n",
      "  -1.26516208e-01  -3.65261151e-03   2.03760322e-02  -1.15660086e-01\n",
      "  -1.12459082e-02  -2.71694176e-03   4.29424085e-02  -1.14199920e-02\n",
      "  -2.10674689e-03   7.55393282e-02  -4.57351655e-02  -2.80568227e-02\n",
      "  -1.10293269e-01   3.79312932e-02   2.12830231e-02  -9.85193998e-04\n",
      "  -4.04955447e-02   2.80246772e-02  -8.57343245e-03   7.37614781e-02\n",
      "   1.42945442e-03   3.82930115e-02   1.89408138e-02   5.01119765e-03\n",
      "   1.53955236e-01   2.22804397e-02  -9.89298709e-03   1.02848575e-01\n",
      "  -5.15679233e-02  -4.98821726e-03   4.03267071e-02  -5.11571504e-02\n",
      "  -4.48506959e-02  -1.33483931e-02   7.02534977e-04   5.71319088e-02\n",
      "   8.21786746e-03  -9.30718333e-03  -5.11409119e-02  -3.17190476e-02\n",
      "  -8.42197984e-02  -2.29573213e-02  -9.43456218e-03  -8.44410881e-02\n",
      "  -7.52052059e-03   9.40788072e-03   1.04055915e-03  -3.79205458e-02\n",
      "   6.69995695e-02   9.41489711e-02  -1.48299523e-02   4.76881079e-02\n",
      "   5.34017235e-02  -6.21343516e-02  -1.26497939e-01  -1.13030206e-02\n",
      "   4.06616554e-02   1.75029598e-02  -3.01819518e-02  -6.95478097e-02\n",
      "   5.15690865e-03  -2.46748589e-02   1.19272294e-02   2.92000617e-03\n",
      "   4.85134237e-02   2.96500716e-02  -2.36916598e-02  -3.65144536e-02\n",
      "   1.13732889e-01   3.33938026e-03  -7.06562772e-02  -7.06619816e-03\n",
      "   1.05281614e-01  -4.80675325e-02   9.01928619e-02  -1.01131633e-01\n",
      "   1.96763650e-02  -8.62577930e-02  -4.97681573e-02   6.87832758e-02\n",
      "   2.48819496e-02   4.01607119e-02  -3.49530950e-02  -3.18254977e-02\n",
      "  -9.91692171e-02  -5.84331602e-02  -1.31063182e-02   2.53437068e-02\n",
      "   5.20803295e-02   4.45366725e-02  -7.45406896e-02   4.76476513e-02\n",
      "   7.10229501e-02  -3.01201511e-02  -1.68831404e-02  -7.94606581e-02\n",
      "  -8.65389481e-02  -5.33752367e-02   5.85081475e-03   6.27366304e-02\n",
      "  -5.33851348e-02   4.52219769e-02  -4.35786620e-02  -2.27289423e-02\n",
      "  -8.86626914e-03   4.87599820e-02  -1.31812125e-01   2.23518647e-02\n",
      "  -3.07947192e-02  -9.26832855e-02  -7.85240263e-04   6.46374151e-02\n",
      "  -1.93218123e-02  -3.95221747e-02  -2.31645517e-02   3.33848060e-03\n",
      "   4.47823331e-02  -1.06069565e-01  -2.29429500e-03  -4.77315187e-02\n",
      "   9.12359506e-02   1.01614352e-02   1.51053555e-02  -4.05994840e-02\n",
      "   5.27386256e-02   4.79689911e-02  -2.49989331e-02  -3.76519337e-02\n",
      "  -5.85748814e-02   4.44749239e-05   4.29092161e-02   6.75603971e-02\n",
      "  -9.16928202e-02  -1.48247615e-01  -9.24448669e-03   9.38039571e-02\n",
      "  -3.45376506e-03   5.39589860e-03   1.12899933e-02  -1.54690649e-02\n",
      "   6.00412637e-02   4.22719233e-02   1.94354579e-02   4.07921262e-02]\n",
      "\n",
      "(300,)\n",
      "\n",
      "[('child', 0.7302687168121338), ('girl', 0.7124578952789307), ('man', 0.66885906457901), ('mother', 0.6269381046295166), ('husband', 0.6248738169670105), ('prostitute', 0.6245359778404236), ('herself', 0.6243365406990051), ('lady', 0.6229467391967773), ('lover', 0.6205604672431946), ('baby', 0.6165746450424194)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model['woman'])\n",
    "print(\"\")\n",
    "print(model['woman'].shape)  # shape of the embedding vector is 300\n",
    "print(\"\")\n",
    "\n",
    "# print most similar words\n",
    "print(model.most_similar(\"woman\"))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.6340517997741699),\n",
       " ('empress', 0.5706495642662048),\n",
       " ('isabella', 0.5673955678939819),\n",
       " ('princess', 0.5533063411712646),\n",
       " ('prince', 0.5482945442199707),\n",
       " ('elizabeth', 0.5417065620422363),\n",
       " ('throne', 0.5405027270317078),\n",
       " ('daughter', 0.5301916003227234),\n",
       " ('monarch', 0.5251233577728271),\n",
       " ('regent', 0.5207557082176208)]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find words most similar to woman and king, but not like man\n",
    "model.most_similar(positive=['woman', 'king'], negative=['man'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.712457985375\n",
      "\n",
      "0.594207825088\n",
      "\n",
      "0.637808347491\n",
      "\n",
      "0.486791184796\n",
      "\n",
      "0.15266906536\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# find the similarity score between 2 words\n",
    "print(model.similarity('girl', 'woman'))\n",
    "print()\n",
    "print(model.similarity('girl', 'man'))\n",
    "print()\n",
    "print(model.similarity('queen', 'king'))\n",
    "print()\n",
    "print(model.similarity('car', 'bus'))\n",
    "print()\n",
    "print(model.similarity('man', 'car'))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
