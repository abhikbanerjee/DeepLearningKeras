{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook works on Word Embeddings using Keras (covers Word2vec for CBOW and Skip Gram models, and Glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "from keras.layers import Merge\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.core import Dense,Reshape, Lambda\n",
    "from keras.models import Sequential\n",
    "import keras.backend as K\n",
    "\n",
    "vocab_size = 5000\n",
    "embedding_size = 300\n",
    "window_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Context model and the Word model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "merge_6 (Merge)              (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 3,000,002\n",
      "Trainable params: 3,000,002\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tkmacl9/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:13: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/Users/tkmacl9/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:14: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, kernel_initializer=\"glorot_uniform\", activation=\"sigmoid\")`\n"
     ]
    }
   ],
   "source": [
    "# word model definition\n",
    "word_model = Sequential()\n",
    "word_model.add(Embedding(vocab_size, embedding_size, embeddings_initializer='glorot_uniform', input_length=1))\n",
    "word_model.add(Reshape((embedding_size,)))\n",
    "\n",
    "# Context model definition\n",
    "context_model = Sequential()\n",
    "context_model.add(Embedding(vocab_size, embedding_size, embeddings_initializer='glorot_uniform', input_length=1))\n",
    "context_model.add(Reshape((embedding_size,)))\n",
    "\n",
    "# add dot product and then a dense layer with sigmoid activation\n",
    "model = Sequential()\n",
    "model.add(Merge([word_model, context_model], mode=\"dot\"))\n",
    "model.add(Dense(1, init=\"glorot_uniform\", activation=\"sigmoid\"))\n",
    "model.compile(loss=\"mean_squared_error\",optimizer=\"adam\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This section reads the embedded layer weights which we are interested in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 300)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_layer = model.layers[0]\n",
    "word_model = merge_layer.layers[0]\n",
    "word_embed_layer = word_model.layers[0]\n",
    "weights = word_embed_layer.get_weights()[0]\n",
    "weights.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This section handles and preprocesses the text to create the skip-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import *\n",
    "from keras.preprocessing.sequence import skipgrams\n",
    "\n",
    "text = \"I love green eggs and ham\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ham': 1, 'eggs': 2, 'and': 3, 'i': 4, 'green': 5, 'love': 6}\n",
      "6\n",
      "{1: 'ham', 2: 'eggs', 3: 'and', 4: 'i', 5: 'green', 6: 'love'}\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# create dictionary from word_to_id and id_to_word\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "word2id = tokenizer.word_index\n",
    "id2word = {v:k for k,v in word2id.items()}\n",
    "print(word2id)\n",
    "print(len(word2id))\n",
    "print(id2word)\n",
    "print(len(id2word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the skipgrams for the given text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 6, 5, 2, 3, 1]\n",
      "length of pairs -  56\n",
      "Length of labels -  56\n",
      "\n",
      "Text is -  I love green eggs and ham\n",
      "\n",
      "love 6  :  ham 1  ->  1\n",
      "and 3  :  eggs 2  ->  0\n",
      "eggs 2  :  green 5  ->  1\n",
      "eggs 2  :  green 5  ->  0\n",
      "eggs 2  :  and 3  ->  0\n",
      "love 6  :  and 3  ->  1\n",
      "love 6  :  green 5  ->  0\n",
      "i 4  :  eggs 2  ->  1\n",
      "and 3  :  ham 1  ->  1\n",
      "love 6  :  i 4  ->  1\n",
      "eggs 2  :  ham 1  ->  0\n",
      "love 6  :  green 5  ->  0\n",
      "i 4  :  eggs 2  ->  0\n",
      "green 5  :  and 3  ->  0\n",
      "eggs 2  :  i 4  ->  1\n",
      "green 5  :  green 5  ->  0\n",
      "ham 1  :  eggs 2  ->  0\n",
      "ham 1  :  ham 1  ->  0\n",
      "ham 1  :  green 5  ->  1\n",
      "love 6  :  and 3  ->  0\n"
     ]
    }
   ],
   "source": [
    "# splits the sentence into list of words and gets the ids for those words from the dict\n",
    "wids = [word2id[w] for w in text_to_word_sequence(text)]\n",
    "print(wids)\n",
    "\n",
    "# create the skipgrams for the texts - the default window_size is 4 for the skipgrams\n",
    "pairs, labels = skipgrams(wids, len(word2id))\n",
    "# pairs, labels = skipgrams(wids, len(word2id), window_size=4)\n",
    "print(\"length of pairs - \", len(pairs))\n",
    "# print(pairs[1])\n",
    "print(\"Length of labels - \", len(labels))\n",
    "print()\n",
    "\n",
    "#print 10 such examples\n",
    "print(\"Text is - \",text)\n",
    "print()\n",
    "for i in range(20):\n",
    "    print(id2word[pairs[i][0]], pairs[i][0],\" : \",\n",
    "         id2word[pairs[i][1]], pairs[i][1],\n",
    "         \" -> \", labels[i] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the CBOW for the given text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_18 (Embedding)     (None, 2, 300)            1500000   \n",
      "_________________________________________________________________\n",
      "lambda_4 (Lambda)            (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 5000)              1505000   \n",
      "=================================================================\n",
      "Total params: 3,005,000\n",
      "Trainable params: 3,005,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_size, embeddings_initializer='glorot_uniform', \n",
    "                   input_length=window_size*2))\n",
    "model.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embedding_size, )))\n",
    "model.add(Dense(vocab_size, kernel_initializer='glorot_uniform', activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights of the embedded layer from the CBOW model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 300)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = model.layers[0].get_weights()[0]\n",
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
