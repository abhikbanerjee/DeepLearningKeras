{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook covers some of the use cases of RNN , LSTM and GRU cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.layers.recurrent import SimpleRNN\n",
    "from keras.models import Sequential\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the text and create a vocab of characters and dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167551\n"
     ]
    }
   ],
   "source": [
    "fin = open(\"/Users/tkmacl9/Desktop/FastAIDLCourse/nbs/KerasDeepLearning/alice_in_wonderland.txt\", \"r\")\n",
    "lines = []\n",
    "for line in fin:\n",
    "    line.strip().lower().replace(\"\\n\",\"\")\n",
    "    line.encode(\"ascii\", errors=\"ignore\")\n",
    "    if len(line)==0:\n",
    "        continue\n",
    "    lines.append(line)\n",
    "fin.close()\n",
    "# lines\n",
    "text = \" \".join(lines)\n",
    "text.replace(\"\\n\",\"\")\n",
    "print(len(text))\n",
    "# print(type(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the dictionary for char to index\n",
    "chars = set([c for c in text])\n",
    "nb_chars = len(chars)\n",
    "char2index = dict((c,i) for i,c in enumerate(chars))\n",
    "index2char = dict((i,c) for i,c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Characters -  86\n",
      "86\n",
      "86\n",
      "\n",
      "{'Z': 0, 'e': 1, 'x': 2, 'd': 44, 'L': 45, '[': 46, 'q': 47, 'P': 48, 'Y': 4, 'G': 49, '.': 5, 'm': 6, 'i': 50, '5': 8, 'T': 51, 'U': 52, 'R': 9, ':': 53, '“': 10, ']': 85, '0': 54, 'r': 11, '#': 55, 'O': 73, 'l': 13, 'y': 56, 'A': 14, '?': 15, 'w': 57, '9': 58, 's': 16, 'J': 18, '7': 59, ';': 60, 'z': 61, 'N': 62, ')': 63, 'a': 64, '6': 21, '_': 22, 'j': 3, 'F': 65, 'H': 23, '’': 19, 'B': 24, '(': 66, '1': 67, 'f': 68, '”': 25, '*': 12, 'M': 26, '\\n': 70, 'Q': 71, 'k': 72, 't': 27, 'W': 28, '!': 74, 'I': 29, 'c': 75, '4': 30, 'h': 76, '-': 31, '@': 32, 'v': 33, '8': 77, '%': 7, 'K': 34, '/': 35, 'E': 78, 'b': 20, 'n': 36, 'V': 37, 'S': 69, 'D': 38, '$': 39, 'p': 40, 'o': 17, 'u': 79, '‘': 80, '3': 81, 'X': 82, '2': 83, ',': 84, 'g': 41, 'C': 42, ' ': 43}\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Characters - \", nb_chars)\n",
    "print(len(char2index))\n",
    "print(len(index2char))\n",
    "print(\"\")\n",
    "print(char2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the text in the form of sequences for the training sets and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167541\n",
      "167541\n",
      "\n",
      "Project Gu - t\n",
      "roject Gut - e\n",
      "oject Gute - n\n",
      "ject Guten - b\n",
      "ect Gutenb - e\n",
      "ct Gutenbe - r\n",
      "t Gutenber - g\n",
      " Gutenberg - ’\n",
      "Gutenberg’ - s\n",
      "utenberg’s -  \n",
      "tenberg’s  - A\n",
      "enberg’s A - l\n",
      "nberg’s Al - i\n",
      "berg’s Ali - c\n",
      "erg’s Alic - e\n",
      "rg’s Alice - ’\n",
      "g’s Alice’ - s\n",
      "’s Alice’s -  \n",
      "s Alice’s  - A\n",
      " Alice’s A - d\n"
     ]
    }
   ],
   "source": [
    "# Define some of the Parameters\n",
    "SEQLENGTH = 10\n",
    "BATCH_SIZE = 128\n",
    "HIDDEN_NEURONS = 128\n",
    "STEP = 1\n",
    "NUM_ITERATIONS = 25\n",
    "NUM_EPOCHS_PER_ITERATION = 1\n",
    "NUM_PREDS_PER_EPOCH = 100\n",
    "\n",
    "input_chars = []\n",
    "label_chars = []\n",
    "\n",
    "for i in range(0, len(text)-SEQLENGTH, STEP):\n",
    "    input_chars.append(text[i: i + SEQLENGTH])\n",
    "    label_chars.append(text[i + SEQLENGTH])\n",
    "\n",
    "# to test the Input characters and the Label Character data look and feel\n",
    "print(len(input_chars))\n",
    "print(len(label_chars))\n",
    "print(\"\")\n",
    "for ind in range(0,20):\n",
    "    print(input_chars[ind] + \" - \"+ label_chars[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the input_chars and the label_chars to vector formats to feed into the RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(167541, 10, 86)\n",
      "(167541, 86)\n",
      "[[False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "   True False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False]\n",
      " [False False False False False False False False False False False  True\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False]\n",
      " [False False False False False False False False False False False False\n",
      "  False False False False False  True False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False]\n",
      " [False False False  True False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False]\n",
      " [False  True False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False]\n",
      " [False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False  True False False False False False False False False\n",
      "  False False]\n",
      " [False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False  True False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False]\n",
      " [False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False  True False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False]\n",
      " [False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False  True False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False]\n",
      " [False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False  True False False False False\n",
      "  False False]]\n",
      " \n",
      "[False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False  True False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False]\n"
     ]
    }
   ],
   "source": [
    "X = np.zeros((len(input_chars), SEQLENGTH, nb_chars), dtype=np.bool)\n",
    "Y = np.zeros(( len(label_chars), nb_chars), dtype=np.bool)\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "\n",
    "# now create the X and the Y vectors which will be used as our datasets to traing the RNN model\n",
    "for i, input_char in enumerate(input_chars):\n",
    "    for j, ch in enumerate(input_char):\n",
    "#         print(j,\" \",ch)\n",
    "        X[i, j, char2index[ch]] = 1\n",
    "    Y[i, char2index[label_chars[i]]] = 1\n",
    "\n",
    "# print one of the values for X and Y vectors\n",
    "print(X[0])\n",
    "print(\" \")\n",
    "print(Y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model with the required parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_1 (SimpleRNN)     (None, 128)               27520     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 86)                11094     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 86)                0         \n",
      "=================================================================\n",
      "Total params: 38,614\n",
      "Trainable params: 38,614\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(SimpleRNN(RNN_CELLS, return_sequences = False,input_shape=(SEQLENGTH, nb_chars), unroll=True))\n",
    "model.add(Dense(nb_chars))\n",
    "model.add(Activation(\"softmax\"))\n",
    "model.summary()\n",
    "\n",
    "# have the model compile\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model and test it , in each epoch unless the output looks great"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Iteration -  1\n",
      "Epoch 1/1\n",
      "167541/167541 [==============================] - 14s - loss: 1.4396    \n",
      "The test sample is -   to hear h\n",
      " to hear his side and she was a little shall her side and she was a little shall her side and she was a little\n",
      "==================================================\n",
      "Iteration -  2\n",
      "Epoch 1/1\n",
      "167541/167541 [==============================] - 14s - loss: 1.4334    \n",
      "The test sample is -  ur pocket?\n",
      "ur pocket?’ said the Caterpillar more the thing the thing the thing the thing the thing the thing the thing th\n",
      "==================================================\n",
      "Iteration -  3\n",
      "Epoch 1/1\n",
      "167541/167541 [==============================] - 15s - loss: 1.4277    \n",
      "The test sample is -  uestion wa\n",
      "uestion was to herself to the sartly of the starked and the poor little sight of the starked and the poor litt\n",
      "==================================================\n",
      "Iteration -  4\n",
      "Epoch 1/1\n",
      "167541/167541 [==============================] - 15s - loss: 1.4233    \n",
      "The test sample is -   she gave \n",
      " she gave of the same as in the same as in the same as in the same as in the same as in the same as in the sam\n",
      "==================================================\n",
      "Iteration -  5\n",
      "Epoch 1/1\n",
      "167541/167541 [==============================] - 15s - loss: 1.4171    \n",
      "The test sample is -  th me! The\n",
      "th me! Then the began to the think you can the door the thing in a look of the stick.\n",
      " \n",
      " ‘I mant in the larte \n",
      "==================================================\n",
      "Iteration -  6\n",
      "Epoch 1/1\n",
      "167541/167541 [==============================] - 15s - loss: 1.4124    \n",
      "The test sample is -  the works!\n",
      "the works! I was a bright in the satter with the game on any reading any reading any reading any reading any r\n",
      "==================================================\n",
      "Iteration -  7\n",
      "Epoch 1/1\n",
      "167541/167541 [==============================] - 14s - loss: 1.4087    \n",
      "The test sample is -  erself in\n",
      "\n",
      "erself in\n",
      " the say it was not they was not they was not they was not they was not they was not they was not th\n",
      "==================================================\n",
      "Iteration -  8\n",
      "Epoch 1/1\n",
      "167541/167541 [==============================] - 15s - loss: 1.4038    \n",
      "The test sample is -   was lit u\n",
      " was lit up again in a more and the the same thing the work in a look of the time the the same thing the work \n",
      "==================================================\n",
      "Iteration -  9\n",
      "Epoch 1/1\n",
      "167541/167541 [==============================] - 15s - loss: 1.4003    \n",
      "The test sample is -  rog; and b\n",
      "rog; and began to the door the work in a some back to the the same the poor little childing to the door the wo\n",
      "==================================================\n",
      "Iteration -  10\n",
      "Epoch 1/1\n",
      "167541/167541 [==============================] - 15s - loss: 1.3957    \n",
      "The test sample is -  hought Ali\n",
      "hought Alice was a little singer the poor and the game and the poor and the game and the poor and the game and\n",
      "==================================================\n",
      "Iteration -  11\n",
      "Epoch 1/1\n",
      "167541/167541 [==============================] - 15s - loss: 1.3928    \n",
      "The test sample is -  large agai\n",
      "large again, and the the same to her face the larte soon of the engled to her gave uent with the eard walk tha\n",
      "==================================================\n",
      "Iteration -  12\n",
      "Epoch 1/1\n",
      "167541/167541 [==============================] - 15s - loss: 1.3891    \n",
      "The test sample is -  t away or\n",
      "\n",
      "t away or\n",
      " the tried to go some the tried to go some the tried to go some the tried to go some the tried to go\n",
      "==================================================\n",
      "Iteration -  13\n",
      "Epoch 1/1\n",
      "167541/167541 [==============================] - 15s - loss: 1.3865    \n",
      "The test sample is -  ntre of th\n",
      "ntre of the eard the poor little simple and the bear looked at the tame of the eard the poor little simple and\n",
      "==================================================\n",
      "Iteration -  14\n",
      "Epoch 1/1\n",
      "167541/167541 [==============================] - 15s - loss: 1.3832    \n",
      "The test sample is -  n her lap \n",
      "n her lap to see she was a little thing the three of the end of the time the the same thing the three of the e\n",
      "==================================================\n",
      "Iteration -  15\n",
      "Epoch 1/1\n",
      "167541/167541 [==============================] - 15s - loss: 1.3796    \n",
      "The test sample is -  f,\n",
      " for th\n",
      "f,\n",
      " for the same the garden so many little sight the same the garden so many little sight the same the garden \n",
      "==================================================\n",
      "Iteration -  16\n",
      "Epoch 1/1\n",
      "167541/167541 [==============================] - 15s - loss: 1.3775    \n",
      "The test sample is -  s a genera\n",
      "s a general cortain a rowing and looked at the stick in the same things and she was a little as she said to he\n",
      "==================================================\n",
      "Iteration -  17\n",
      "Epoch 1/1\n",
      "167541/167541 [==============================] - 15s - loss: 1.3742    \n",
      "The test sample is -  ned, and a\n",
      "ned, and a little sight it a little sight it a little sight it a little sight it a little sight it a little si\n",
      "==================================================\n",
      "Iteration -  18\n",
      "Epoch 1/1\n",
      "167541/167541 [==============================] - 15s - loss: 1.3713    \n",
      "The test sample is -  k?’\n",
      " \n",
      " ‘Co\n",
      "k?’\n",
      " \n",
      " ‘Come of the end of the white thought Alice was a little side and the back to the the same she was a li\n",
      "==================================================\n",
      "Iteration -  19\n",
      "Epoch 1/1\n",
      "167541/167541 [==============================] - 15s - loss: 1.3689    \n",
      "The test sample is -  ’s marked \n",
      "’s marked in an and she was an into all the large more that the large more that the large more that the large \n",
      "==================================================\n",
      "Iteration -  20\n",
      "Epoch 1/1\n",
      "167541/167541 [==============================] - 15s - loss: 1.3664    \n",
      "The test sample is -  ave of Hea\n",
      "ave of Hearts at the same she was and shorter sore it was a little side and the baby the whole beginned to the\n",
      "==================================================\n",
      "Iteration -  21\n",
      "Epoch 1/1\n",
      "167541/167541 [==============================] - 15s - loss: 1.3632    \n",
      "The test sample is -  eping so c\n",
      "eping so cats at the same the court, and she was a say the Gryphon, and the court, and she was a say the Gryph\n",
      "==================================================\n",
      "Iteration -  22\n",
      "Epoch 1/1\n",
      "167541/167541 [==============================] - 15s - loss: 1.3608    \n",
      "The test sample is -  st telesco\n",
      "st telescopred, ‘I must be a book and the court in a mouse, and the permouse of the words and the permouse of \n",
      "==================================================\n",
      "Iteration -  23\n",
      "Epoch 1/1\n",
      "167541/167541 [==============================] - 16s - loss: 1.3584    \n",
      "The test sample is -  eads below\n",
      "eads belowed to the short in a little copy out of the short in a little copy out of the short in a little copy\n",
      "==================================================\n",
      "Iteration -  24\n",
      "Epoch 1/1\n",
      "167541/167541 [==============================] - 15s - loss: 1.3561    \n",
      "The test sample is -  d the yout\n",
      "d the youted a little she said to herself some time was a langer it all the copyright her head to herself some\n",
      "==================================================\n",
      "Iteration -  25\n",
      "Epoch 1/1\n",
      "167541/167541 [==============================] - 15s - loss: 1.3543    \n",
      "The test sample is -  d went on \n",
      "d went on the tried to her have a porrouthed the poor for the thing the same thing the same thing the same thi\n"
     ]
    }
   ],
   "source": [
    "# now train the model for the required number of iterations and repeat the process\n",
    "# by testing and getting the next character and appending it to the next input\n",
    "\n",
    "for iteration in range(NUM_ITERATIONS):\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Iteration - \", iteration+1)\n",
    "    model.fit(X, Y, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS_PER_ITERATION)\n",
    "    \n",
    "    # this section is to test the output at every iteration, and see how it starts making sense\n",
    "    test_idx = np.random.randint(len(input_chars))\n",
    "    test_sample = input_chars[test_idx]\n",
    "    print(\"The test sample is - \", test_sample)\n",
    "    print(test_sample, end=\"\")\n",
    "    for i in range(NUM_PREDS_PER_EPOCH):\n",
    "        X_test = np.zeros((1, SEQLENGTH, nb_chars), dtype=np.bool)\n",
    "        for i, ch in enumerate(test_sample):\n",
    "            X_test[0, i, char2index[ch]] = 1\n",
    "        pred = model.predict(X_test, verbose=0)[0]\n",
    "        y_pred = index2char[np.argmax(pred)]\n",
    "        print(y_pred, end=\"\")\n",
    "        test_sample = test_sample[1:] + y_pred\n",
    "    print()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
